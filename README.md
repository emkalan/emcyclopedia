# emcyclopedia
A proof-of-concept LSTM text generation model trained after WikiText-2.

This is a basic LSTM text generation model, trained on the WikiText-2 dataset. Itâ€™s not optimized for performance and is more of a rough prototype than a polished product. Expect subpar output, long training times, and occasional moments of total incoherence. Ideal for those looking to see what happens when you throw a bunch of text into a neural network with minimal tweaking. Use at your own risk.

Currently in development with a focus on refining output quality and efficiency.
