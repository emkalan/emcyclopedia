{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notice\n",
        "The EmCyclopedia is a proof of concept. Emma's Encyclopedia from HuggingFace wikipedia corpus. The Worst (Soon-To-Be) Neural Net of All time.\n",
        "\n",
        "By the time it is complete it will have anywhere from 3 to 30 million parameters. (Eek!)\n",
        "\n",
        "Specifically this \"v3.00\" file exists to **finalize the project from dummy training pipeline**. What you are seeing is the 11th version of EmCyclopedia.\n",
        "\n",
        "## How to use it?\n",
        "First run the cell below to install all dependencies. This may take 5-10 minutes. (A long time!)\n",
        "\n",
        "After that the dummy training pipeline cell will do the rest.\n",
        "\n",
        "No GUI sorry. Maybe later.\n",
        "\n",
        "Params for generation need to be added again once I actually have text generation\n",
        "\n",
        "## TODOs\n",
        "\n",
        "- Fix dummy training pipeline.\n",
        "- Implement backpropagation. (Eek!)\n",
        "- Add control for top P, min token, max token (instead of prior forced token output).\n",
        "\n",
        "## Tentative Pipeline\n",
        "\n",
        "- Use spaCy for word vectors. It returns 300-dimensional vectors so I do not have to count words like a caveman. But that's a lotta dimensions so I so scared.\n",
        "- Training from scratch.\n",
        "\n",
        "## Things to look into\n",
        "- spaCy documentation\n",
        "- PyTorch, what it do! Yop yop yop yop."
      ],
      "metadata": {
        "id": "H0CKBNjzyU5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install tqdm\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJFoUR_5y1WN",
        "outputId": "1aea2687-a0b1-4da4-b39a-31801dfbe973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "print(\"Loading Wikitext dataset...\")\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\", streaming=True)\n",
        "print(\"Yop yop! Wikipedia dataset loaded.\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# --- AGGRESSIVE SAMPLING ---\n",
        "small_ds = ds.take(10000)  # Take only 1000 examples\n",
        "# --- END AGGRESSIVE SAMPLING ---\n",
        "\n",
        "def tokenize_text(text, word_to_index):\n",
        "  tokens = [token.text.lower() for token in nlp(text) if token.is_alpha]\n",
        "  return [word_to_index.get(token, 0) for token in tokens]\n",
        "\n",
        "class emCyclopedia_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Predict next word\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Tensor of shape (batch_size, sequence_length) containing the word indices.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, sequence_length, vocab_size) containing the logits.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, sequence_length, embedding_dim)\n",
        "        lstm_out, _ = self.lstm(embedded)     # (batch_size, sequence_length, hidden_dim)\n",
        "        logits = self.fc(lstm_out)          # (batch_size, sequence_length, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, top_p=0.9, temperature=1.0, max_length=50):\n",
        "        generated_tokens = []\n",
        "        for _ in range(max_length):\n",
        "            logits = self.forward(input_ids)  # Get raw scores\n",
        "            logits = logits[:, -1, :]  # Focus on the last token\n",
        "            logits = logits / temperature  # Apply temperature scaling\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert to probabilities\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "            # Top-p nucleus filtering\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            cutoff_index = (cumulative_probs > top_p).nonzero(as_tuple=True)[1][0].item()\n",
        "            filtered_probs = sorted_probs[:, :cutoff_index + 1] #remember to index both dimensions!\n",
        "            filtered_indices = sorted_indices[:, :cutoff_index + 1]\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            next_token_index = torch.multinomial(filtered_probs, 1).item() #extract the index first!\n",
        "            next_token = filtered_indices[0, next_token_index]  # Now index correctly.\n",
        "            generated_tokens.append(next_token.item()) #append the item\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token.item()]], dtype=torch.long).to(input_ids.device)], dim=1)  # Append new token, and make sure its the item!\n",
        "\n",
        "        return generated_tokens\n",
        "\n",
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, data, word_to_index, max_examples=10000, window_size=3):\n",
        "        self.data = []\n",
        "        self.window_size = window_size #store the window size\n",
        "        count = 0\n",
        "        for example in data:\n",
        "            text = example['text']\n",
        "            tokenized = tokenize_text(text, word_to_index)\n",
        "            # Create input/target pairs\n",
        "            for i in range(0, len(tokenized) - self.window_size, 1): #iterate correctly\n",
        "                context = torch.tensor(tokenized[i : i + self.window_size ], dtype=torch.long)\n",
        "                target  = torch.tensor(tokenized[i + 1: i + self.window_size + 1], dtype=torch.long) #get all the targets\n",
        "                count += 1\n",
        "                self.data.append((context, target))\n",
        "                if count >= max_examples:\n",
        "                  return\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# --- INSERT THIS BLOCK ---\n",
        "print(\"Tokenizing and building vocabulary...\")\n",
        "all_words = []\n",
        "total_examples = 0\n",
        "\n",
        "for example in small_ds:\n",
        "    start_time = time.time()\n",
        "    total_examples += 1\n",
        "    text = example['text']\n",
        "    tokens = [token.text.lower() for token in nlp(text) if token.is_alpha]\n",
        "    all_words.extend(tokens)\n",
        "    end_time = time.time()\n",
        "    if total_examples % 100 == 0:  # Frequent updates\n",
        "        print(f\"Processed {total_examples} examples. Current all_words size: {len(all_words)}. Time per example: {(end_time - start_time):.4f} seconds\")\n",
        "\n",
        "print(f\"Tokenization complete. Total examples processed: {total_examples}\")\n",
        "# --- END OF INSERTED BLOCK ---\n",
        "\n",
        "# Create vocabulary\n",
        "word_counts = Counter(all_words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "vocab_size = len(vocab)\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "# --- AGGRESSIVE SAMPLING (WikiDataset) ---\n",
        "train_dataset = WikiDataset(small_ds, word_to_index, max_examples=10000, window_size=100)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# --- END AGGRESSIVE SAMPLING ---\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = emCyclopedia_LSTM(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzkKx9qSzhVS",
        "outputId": "ebcfce0c-23f2-4b13-8079-033daf8a9175"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wikitext dataset...\n",
            "Yop yop! Wikipedia dataset loaded.\n",
            "Tokenizing and building vocabulary...\n",
            "Processed 100 examples. Current all_words size: 5425. Time per example: 0.0039 seconds\n",
            "Processed 200 examples. Current all_words size: 8325. Time per example: 0.0036 seconds\n",
            "Processed 300 examples. Current all_words size: 9798. Time per example: 0.0288 seconds\n",
            "Processed 400 examples. Current all_words size: 13202. Time per example: 0.0002 seconds\n",
            "Processed 500 examples. Current all_words size: 18099. Time per example: 0.0390 seconds\n",
            "Processed 600 examples. Current all_words size: 22528. Time per example: 0.0037 seconds\n",
            "Processed 700 examples. Current all_words size: 28383. Time per example: 0.0002 seconds\n",
            "Processed 800 examples. Current all_words size: 34369. Time per example: 0.0033 seconds\n",
            "Processed 900 examples. Current all_words size: 39099. Time per example: 0.0001 seconds\n",
            "Processed 1000 examples. Current all_words size: 43955. Time per example: 0.0094 seconds\n",
            "Processed 1100 examples. Current all_words size: 48053. Time per example: 0.0112 seconds\n",
            "Processed 1200 examples. Current all_words size: 53029. Time per example: 0.0001 seconds\n",
            "Processed 1300 examples. Current all_words size: 59622. Time per example: 0.0150 seconds\n",
            "Processed 1400 examples. Current all_words size: 63394. Time per example: 0.0002 seconds\n",
            "Processed 1500 examples. Current all_words size: 69496. Time per example: 0.0153 seconds\n",
            "Processed 1600 examples. Current all_words size: 74049. Time per example: 0.0001 seconds\n",
            "Processed 1700 examples. Current all_words size: 78824. Time per example: 0.0002 seconds\n",
            "Processed 1800 examples. Current all_words size: 84526. Time per example: 0.0267 seconds\n",
            "Processed 1900 examples. Current all_words size: 89951. Time per example: 0.0001 seconds\n",
            "Processed 2000 examples. Current all_words size: 95407. Time per example: 0.0002 seconds\n",
            "Processed 2100 examples. Current all_words size: 102244. Time per example: 0.0097 seconds\n",
            "Processed 2200 examples. Current all_words size: 107524. Time per example: 0.0056 seconds\n",
            "Processed 2300 examples. Current all_words size: 111141. Time per example: 0.0002 seconds\n",
            "Processed 2400 examples. Current all_words size: 115959. Time per example: 0.0031 seconds\n",
            "Processed 2500 examples. Current all_words size: 120586. Time per example: 0.0001 seconds\n",
            "Processed 2600 examples. Current all_words size: 123470. Time per example: 0.0002 seconds\n",
            "Processed 2700 examples. Current all_words size: 127864. Time per example: 0.0177 seconds\n",
            "Processed 2800 examples. Current all_words size: 133723. Time per example: 0.0001 seconds\n",
            "Processed 2900 examples. Current all_words size: 136552. Time per example: 0.0034 seconds\n",
            "Processed 3000 examples. Current all_words size: 137608. Time per example: 0.0031 seconds\n",
            "Processed 3100 examples. Current all_words size: 139299. Time per example: 0.0002 seconds\n",
            "Processed 3200 examples. Current all_words size: 146728. Time per example: 0.0171 seconds\n",
            "Processed 3300 examples. Current all_words size: 153141. Time per example: 0.0038 seconds\n",
            "Processed 3400 examples. Current all_words size: 157219. Time per example: 0.0001 seconds\n",
            "Processed 3500 examples. Current all_words size: 160513. Time per example: 0.0173 seconds\n",
            "Processed 3600 examples. Current all_words size: 164552. Time per example: 0.0051 seconds\n",
            "Processed 3700 examples. Current all_words size: 168896. Time per example: 0.0185 seconds\n",
            "Processed 3800 examples. Current all_words size: 174822. Time per example: 0.0143 seconds\n",
            "Processed 3900 examples. Current all_words size: 177750. Time per example: 0.0037 seconds\n",
            "Processed 4000 examples. Current all_words size: 183191. Time per example: 0.0036 seconds\n",
            "Processed 4100 examples. Current all_words size: 186359. Time per example: 0.0055 seconds\n",
            "Processed 4200 examples. Current all_words size: 189263. Time per example: 0.0034 seconds\n",
            "Processed 4300 examples. Current all_words size: 193807. Time per example: 0.0113 seconds\n",
            "Processed 4400 examples. Current all_words size: 196943. Time per example: 0.0098 seconds\n",
            "Processed 4500 examples. Current all_words size: 200971. Time per example: 0.0002 seconds\n",
            "Processed 4600 examples. Current all_words size: 204551. Time per example: 0.0063 seconds\n",
            "Processed 4700 examples. Current all_words size: 209678. Time per example: 0.0001 seconds\n",
            "Processed 4800 examples. Current all_words size: 211337. Time per example: 0.0001 seconds\n",
            "Processed 4900 examples. Current all_words size: 215197. Time per example: 0.0063 seconds\n",
            "Processed 5000 examples. Current all_words size: 218868. Time per example: 0.0041 seconds\n",
            "Processed 5100 examples. Current all_words size: 222139. Time per example: 0.0100 seconds\n",
            "Processed 5200 examples. Current all_words size: 226755. Time per example: 0.0001 seconds\n",
            "Processed 5300 examples. Current all_words size: 231207. Time per example: 0.0001 seconds\n",
            "Processed 5400 examples. Current all_words size: 234661. Time per example: 0.0251 seconds\n",
            "Processed 5500 examples. Current all_words size: 238954. Time per example: 0.0097 seconds\n",
            "Processed 5600 examples. Current all_words size: 244323. Time per example: 0.0051 seconds\n",
            "Processed 5700 examples. Current all_words size: 247223. Time per example: 0.0088 seconds\n",
            "Processed 5800 examples. Current all_words size: 250462. Time per example: 0.0053 seconds\n",
            "Processed 5900 examples. Current all_words size: 253472. Time per example: 0.0002 seconds\n",
            "Processed 6000 examples. Current all_words size: 257473. Time per example: 0.0001 seconds\n",
            "Processed 6100 examples. Current all_words size: 262889. Time per example: 0.0038 seconds\n",
            "Processed 6200 examples. Current all_words size: 267019. Time per example: 0.0158 seconds\n",
            "Processed 6300 examples. Current all_words size: 272246. Time per example: 0.0001 seconds\n",
            "Processed 6400 examples. Current all_words size: 276929. Time per example: 0.0124 seconds\n",
            "Processed 6500 examples. Current all_words size: 285982. Time per example: 0.0179 seconds\n",
            "Processed 6600 examples. Current all_words size: 291310. Time per example: 0.0154 seconds\n",
            "Processed 6700 examples. Current all_words size: 294919. Time per example: 0.0032 seconds\n",
            "Processed 6800 examples. Current all_words size: 300998. Time per example: 0.0040 seconds\n",
            "Processed 6900 examples. Current all_words size: 306128. Time per example: 0.0206 seconds\n",
            "Processed 7000 examples. Current all_words size: 309412. Time per example: 0.0038 seconds\n",
            "Processed 7100 examples. Current all_words size: 316045. Time per example: 0.0002 seconds\n",
            "Processed 7200 examples. Current all_words size: 318328. Time per example: 0.0001 seconds\n",
            "Processed 7300 examples. Current all_words size: 322635. Time per example: 0.0142 seconds\n",
            "Processed 7400 examples. Current all_words size: 327501. Time per example: 0.0001 seconds\n",
            "Processed 7500 examples. Current all_words size: 331251. Time per example: 0.0096 seconds\n",
            "Processed 7600 examples. Current all_words size: 337165. Time per example: 0.0001 seconds\n",
            "Processed 7700 examples. Current all_words size: 341473. Time per example: 0.0035 seconds\n",
            "Processed 7800 examples. Current all_words size: 346930. Time per example: 0.0001 seconds\n",
            "Processed 7900 examples. Current all_words size: 351335. Time per example: 0.0002 seconds\n",
            "Processed 8000 examples. Current all_words size: 357967. Time per example: 0.0116 seconds\n",
            "Processed 8100 examples. Current all_words size: 361789. Time per example: 0.0002 seconds\n",
            "Processed 8200 examples. Current all_words size: 367154. Time per example: 0.0001 seconds\n",
            "Processed 8300 examples. Current all_words size: 371111. Time per example: 0.0089 seconds\n",
            "Processed 8400 examples. Current all_words size: 375874. Time per example: 0.0002 seconds\n",
            "Processed 8500 examples. Current all_words size: 382622. Time per example: 0.0037 seconds\n",
            "Processed 8600 examples. Current all_words size: 387175. Time per example: 0.0121 seconds\n",
            "Processed 8700 examples. Current all_words size: 391474. Time per example: 0.0001 seconds\n",
            "Processed 8800 examples. Current all_words size: 395895. Time per example: 0.0002 seconds\n",
            "Processed 8900 examples. Current all_words size: 398767. Time per example: 0.0002 seconds\n",
            "Processed 9000 examples. Current all_words size: 403561. Time per example: 0.0064 seconds\n",
            "Processed 9100 examples. Current all_words size: 407741. Time per example: 0.0095 seconds\n",
            "Processed 9200 examples. Current all_words size: 413423. Time per example: 0.0202 seconds\n",
            "Processed 9300 examples. Current all_words size: 417075. Time per example: 0.0200 seconds\n",
            "Processed 9400 examples. Current all_words size: 421243. Time per example: 0.0129 seconds\n",
            "Processed 9500 examples. Current all_words size: 427680. Time per example: 0.0036 seconds\n",
            "Processed 9600 examples. Current all_words size: 432563. Time per example: 0.0001 seconds\n",
            "Processed 9700 examples. Current all_words size: 436555. Time per example: 0.0035 seconds\n",
            "Processed 9800 examples. Current all_words size: 440164. Time per example: 0.0001 seconds\n",
            "Processed 9900 examples. Current all_words size: 443192. Time per example: 0.0001 seconds\n",
            "Processed 10000 examples. Current all_words size: 448006. Time per example: 0.0033 seconds\n",
            "Tokenization complete. Total examples processed: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "valid_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\", streaming=True)\n",
        "valid_dataset = WikiDataset(valid_ds, word_to_index, max_examples=10000, window_size=100) # Use the same parameters as training\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)  # shuffle=False for evaluation\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "# --- Assuming you have your model, optimizer, criterion, train_loader, and valid_loader defined ---\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, epochs=100, patience=10): # Added patience\n",
        "    model.train()\n",
        "    best_valid_loss = float('inf')  # Initialize best validation loss\n",
        "    patience_counter = 0 # counter for early stopping\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- Training Phase ---\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_train_loss = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            if (batch_num + 1) % 10 == 0:  # Print progress every 10 batches\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_num+1}, Loss: {loss.item():.4f}\")\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        total_valid_loss = 0\n",
        "        with torch.no_grad():  # Disable gradient calculation during validation\n",
        "            for inputs, targets in valid_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "                total_valid_loss += loss.item()\n",
        "\n",
        "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
        "        valid_losses.append(avg_valid_loss)\n",
        "        print(f\"Epoch {epoch+1}, Valid Loss: {avg_valid_loss:.4f}\")\n",
        "\n",
        "        # --- Early Stopping ---\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model (optional, but recommended)\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break # Stop if it has not gotten better.\n",
        "\n",
        "# --- END AGGRESSIVE TRAINING AND VALIDATION ---\n",
        "train_model(model, train_loader, valid_loader, epochs=100, patience=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9hzUpqt_SKH",
        "outputId": "5abd8f81-478b-4d14-9808-bfabcce9c9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 10, Loss: 6.6539\n",
            "Epoch 1, Batch 20, Loss: 6.5807\n",
            "Epoch 1, Batch 30, Loss: 6.7634\n",
            "Epoch 1, Batch 40, Loss: 6.6724\n",
            "Epoch 1, Batch 50, Loss: 6.6151\n",
            "Epoch 1, Batch 60, Loss: 6.6746\n",
            "Epoch 1, Batch 70, Loss: 6.6645\n",
            "Epoch 1, Batch 80, Loss: 6.5450\n",
            "Epoch 1, Batch 90, Loss: 6.7582\n",
            "Epoch 1, Batch 100, Loss: 6.6609\n",
            "Epoch 1, Batch 110, Loss: 6.4986\n",
            "Epoch 1, Batch 120, Loss: 6.6664\n",
            "Epoch 1, Batch 130, Loss: 6.7067\n",
            "Epoch 1, Batch 140, Loss: 6.5908\n",
            "Epoch 1, Batch 150, Loss: 6.7142\n",
            "Epoch 1, Batch 160, Loss: 6.5322\n",
            "Epoch 1, Batch 170, Loss: 6.5708\n",
            "Epoch 1, Batch 180, Loss: 6.5127\n",
            "Epoch 1, Batch 190, Loss: 6.7598\n",
            "Epoch 1, Batch 200, Loss: 6.5875\n",
            "Epoch 1, Batch 210, Loss: 6.7177\n",
            "Epoch 1, Batch 220, Loss: 6.5282\n",
            "Epoch 1, Batch 230, Loss: 6.5499\n",
            "Epoch 1, Batch 240, Loss: 6.5765\n",
            "Epoch 1, Batch 250, Loss: 6.4745\n",
            "Epoch 1, Batch 260, Loss: 6.7351\n",
            "Epoch 1, Batch 270, Loss: 6.6152\n",
            "Epoch 1, Batch 280, Loss: 6.5499\n",
            "Epoch 1, Batch 290, Loss: 6.5228\n",
            "Epoch 1, Batch 300, Loss: 6.4134\n",
            "Epoch 1, Batch 310, Loss: 6.4997\n",
            "Epoch 1, Batch 320, Loss: 6.5276\n",
            "Epoch 1, Batch 330, Loss: 6.4713\n",
            "Epoch 1, Batch 340, Loss: 6.4881\n",
            "Epoch 1, Batch 350, Loss: 6.2916\n",
            "Epoch 1, Batch 360, Loss: 6.6101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eVmU6pyYJlTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.plot(epochs, train_losses, marker='o', linestyle='-', label='Training Loss')\n",
        "plt.plot(epochs, valid_losses, marker='x', linestyle='-', label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss vs. Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()  # Add a legend to differentiate the lines\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JzuvFrjD8tXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (after training)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Start with a seed sequence (e.g., \"The cat sat\")\n",
        "seed_text = \"The cat sat\"\n",
        "seed_tokens = tokenize_text(seed_text, word_to_index)\n",
        "input_ids = torch.tensor([seed_tokens], dtype=torch.long).to(device)\n",
        "\n",
        "# Generate text\n",
        "generated_tokens = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# Convert tokens back to words\n",
        "generated_text = \" \".join([vocab[token] for token in generated_tokens])\n",
        "\n",
        "print(f\"Seed text: {seed_text}\")\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNtJwtgu0ZHY",
        "outputId": "d23c48dd-0a58-4c82-fa23-c7075b85b639"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed text: The cat sat\n",
            "Generated text: fresno both returned from previous entries along with valkyria chronicles ii while it retained the standard features of the series of linear missions gradually unlocked as maps that remain unaltered unless otherwise dictated by the darcsen heavy weapons specialist who seeks revenge against the valkyria who destroyed her home and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"emcyclopedia_v3.pt\")\n",
        "print(\"Model saved.\")"
      ],
      "metadata": {
        "id": "YEEp28Bc0b9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"emcyclopedia_v3.pt\"))\n",
        "model.eval()  # Put it in evaluation mode\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "id": "5neuNkQ30d5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "vTH9RRASyQmE",
        "outputId": "623e32a6-0d5b-4a18-d8b0-c6bf4d7e9947"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[64, -1]' is invalid for input of size 30",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d11661705f11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d11661705f11>\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(data, seq_length, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Define RNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, -1]' is invalid for input of size 30"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 128  # Size of hidden layer\n",
        "num_layers = 2     # Number of RNN layers\n",
        "seq_length = 30    # Number of characters per training sequence\n",
        "batch_size = 64    # Batch size\n",
        "learning_rate = 0.002\n",
        "num_epochs = 10\n",
        "\n",
        "# Sample text data (replace with actual Wikipedia text)\n",
        "text = \"The quick brown fox jumps over the lazy dog. \" * 100\n",
        "chars = list(set(text))  # Unique characters\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Convert text to numerical format\n",
        "data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n",
        "\n",
        "def get_batches(data, seq_length, batch_size):\n",
        "    n = (len(data) - 1) // (seq_length * batch_size)\n",
        "    for i in range(0, n * batch_size * seq_length, seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+1:i+seq_length+1]\n",
        "        yield x.view(batch_size, -1), y.view(batch_size, -1)\n",
        "\n",
        "# Define RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embed(x)\n",
        "        out, h = self.rnn(x, h)\n",
        "        out = self.fc(out.reshape(out.size(0) * out.size(1), -1))\n",
        "        return out, h\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = RNNModel(vocab_size, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    h = torch.zeros(num_layers, batch_size, hidden_size)  # Initialize hidden state\n",
        "    for x, y in get_batches(data, seq_length, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        output, h = model(x, h.detach())\n",
        "        loss = criterion(output, y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Generate text\n",
        "def generate_text(start_str, length=100):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_str], dtype=torch.long).unsqueeze(0)\n",
        "    h = torch.zeros(num_layers, 1, hidden_size)\n",
        "    result = start_str\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output, h = model(input_seq, h)\n",
        "            pred_idx = torch.argmax(output[-1]).item()\n",
        "            result += idx_to_char[pred_idx]\n",
        "            input_seq = torch.tensor([[pred_idx]])\n",
        "    return result\n",
        "\n",
        "print(generate_text(\"The quick \", 200))\n"
      ]
    }
  ]
}